<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator>
  <link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2023-10-09T02:31:31-07:00</updated>
  <id>http://localhost:4000/</id>

  
    <title type="html">Data Engineer</title>
  

  
    <subtitle>Hi. This is my site where i can showcase my work and expertise in the data engineering field. I have 5 Years of experience and always looking to expand my skillset!</subtitle>
  

  

  
  
    <entry>
      <title type="html">Project 1</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 1" />
      <published>2023-09-20T00:00:00-07:00</published>
      <updated>2023-09-20T00:00:00-07:00</updated>
      <id>http://localhost:4000/project-1</id>
      <content type="html" xml:base="http://localhost:4000/">&lt;h1 id=&quot;aws-lambda-function-earthquake-dashboard-pipeline&quot;&gt;AWS Lambda Function: Earthquake Dashboard Pipeline&lt;/h1&gt;

&lt;iframe src=&quot;https://public.tableau.com/views/earthquakes_16956986619940/Sheet1?:showVizHome=no&amp;amp;:embed=true&quot; width=&quot;800&quot; height=&quot;400&quot; value=&quot;:original_view=yes&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;This AWS Lambda function is designed to automate the process of fetching earthquake data from the USGS Earthquake Catalog, transforming it into a suitable format, and then upserting (inserting or updating) the data into a MySQL database table. The function also ensures that duplicate data is not inserted into the database.&lt;/p&gt;

&lt;h2 id=&quot;steps&quot;&gt;Steps&lt;/h2&gt;

&lt;p&gt;Here’s a detailed breakdown of what the Lambda function does:&lt;/p&gt;

&lt;h2 id=&quot;step-1-data-retrieval&quot;&gt;Step 1: Data Retrieval&lt;/h2&gt;
&lt;p&gt;The Lambda function starts by retrieving earthquake data from the USGS Earthquake Catalog. It fetches the data in CSV format using an HTTP GET request.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;earthquake_url = &apos;https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_day.csv&apos;
s = requests.get(earthquake_url).content
earthquake_df = pd.read_csv(io.StringIO(s.decode(&apos;utf-8&apos;)))


&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;step-2-data-transformation&quot;&gt;Step 2: Data Transformation&lt;/h1&gt;

&lt;p&gt;The fetched data is converted into a Pandas DataFrame, making it easier to work with. Additionally, a new column called ‘hash_column’ is created in the DataFrame by combining all other columns and hashing the resulting string using SHA-256.&lt;/p&gt;

&lt;h1 id=&quot;step-3-data-preprocessing&quot;&gt;Step 3: Data Preprocessing&lt;/h1&gt;
&lt;p&gt;Any NaN (Not a Number) values in the DataFrame are filled with ‘0.00’ to ensure consistency.&lt;/p&gt;

&lt;h1 id=&quot;step-4-data-type-mapping&quot;&gt;Step 4: Data Type Mapping&lt;/h1&gt;
&lt;p&gt;A mapping of Pandas data types to MySQL data types is defined to facilitate the subsequent database interaction.&lt;/p&gt;

&lt;h1 id=&quot;step-5-database-connection&quot;&gt;Step 5: Database Connection&lt;/h1&gt;
&lt;p&gt;The Lambda function establishes a connection to a MySQL database using environment variables for the database endpoint, username, and password.&lt;/p&gt;

&lt;h1 id=&quot;step-6-check-for-existing-data&quot;&gt;Step 6: Check for Existing Data&lt;/h1&gt;
&lt;p&gt;An SQL query is executed to retrieve hash values from an existing MySQL table in the “portfolio” database. These hash values represent the data that is already present in the database.&lt;/p&gt;

&lt;h1 id=&quot;step-7-data-upsert&quot;&gt;Step 7: Data Upsert&lt;/h1&gt;
&lt;p&gt;The function then identifies new records in the DataFrame by comparing the hash values in the DataFrame with the retrieved hash values from the MySQL table. It filters the DataFrame to keep only the new records that are not present in the MySQL table.&lt;/p&gt;

&lt;h1 id=&quot;step-8-data-insertion&quot;&gt;Step 8: Data Insertion&lt;/h1&gt;
&lt;p&gt;SQL INSERT statements are dynamically generated based on the data types and column names obtained earlier. The Lambda function iterates through the filtered DataFrame and inserts the new records into the MySQL table.&lt;/p&gt;

&lt;h1 id=&quot;step-9-response&quot;&gt;Step 9: Response&lt;/h1&gt;
&lt;p&gt;Finally, the Lambda function returns a JSON response indicating the status of execution. If the function completes successfully, it returns a status code of 200 and a message of “Done.” If an exception occurs, it returns a status code of 404 along with an error message.&lt;/p&gt;

&lt;p&gt;This Lambda function streamlines the process of acquiring earthquake data and ensuring that it’s efficiently upserted into a MySQL database, maintaining data integrity and avoiding duplicate entries. It serves as an example of automation and data synchronization in a serverless environment.&lt;/p&gt;

&lt;h1 id=&quot;code&quot;&gt;Code&lt;/h1&gt;

&lt;p&gt;You Can find the full code &lt;a href=&quot;https://github.com/Nicholasphom/Nicholasphom.github.io/blob/main/PortfolioCode/Project1/lambda_function.py&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="ETL Pipeline/BI" />
      

      

      
        <summary type="html">AWS Lambda Function: Earthquake Dashboard Pipeline</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Project 2</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 2" />
      <published>2023-07-17T00:00:00-07:00</published>
      <updated>2023-07-17T00:00:00-07:00</updated>
      <id>http://localhost:4000/project-2</id>
      <content type="html" xml:base="http://localhost:4000/">&lt;h1 id=&quot;docker-compose-for-hadoop-spark-and-airflow&quot;&gt;Docker Compose for Hadoop, Spark, and Airflow&lt;/h1&gt;
&lt;p&gt;This Docker Compose configuration is designed to set up a development environment for Hadoop, Spark, and Apache Airflow. It orchestrates various services and containers, making it easier to run and manage these components together.
This is meant for you to run a sanitized develop enviorment and is a work in progress
You can find the repo &lt;a href=&quot;https://github.com/Nicholasphom/datapipeline-docker/tree/master&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="Docker/Cloud Architect" />
      

      

      
        <summary type="html">Docker Compose for Hadoop, Spark, and Airflow This Docker Compose configuration is designed to set up a development environment for Hadoop, Spark, and Apache Airflow. It orchestrates various services and containers, making it easier to run and manage these components together. This is meant for you to run a sanitized develop enviorment and is a work in progress You can find the repo here</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Project 4</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 4" />
      <published>2023-05-01T00:00:00-07:00</published>
      <updated>2023-05-01T00:00:00-07:00</updated>
      <id>http://localhost:4000/project-4</id>
      <content type="html" xml:base="http://localhost:4000/">&lt;h1 id=&quot;sacramento-county-spark-data-pipeline-project&quot;&gt;Sacramento County Spark Data Pipeline Project&lt;/h1&gt;

&lt;p&gt;In this project, I was brought on as a data engineering consultant to help with the initiative to redo Sacramento County’s data warehouse. This was a 4-month project. I was tasked mainly to architect the on-premise resources as well as make plans to port over to AWS in the future.&lt;/p&gt;

&lt;h2 id=&quot;previous-architecture&quot;&gt;Previous Architecture&lt;/h2&gt;

&lt;p&gt;Previously, they only had an Oracle database replicated across 3 servers that connect directly to their applications. They shared a lot of data loads between their data analyst, front-end engineers, as well as the public-facing website where users can see their data. they experienced a lot of downtime as well as performance issues depending on amount of users using database resources. The cost was a concern also as more Oracle servers required more licenses.&lt;/p&gt;

&lt;h2 id=&quot;proposed-solution&quot;&gt;Proposed Solution&lt;/h2&gt;

&lt;p&gt;With the picture referenced above, I proposed a solution around the use of Python, Spark and other open-source MySql Databases. First, we keep the Oracle database but we do ETL and CDC Extractions to a file system, in this case, HDFS was used. The file format stored is in parquet since we can bake the schema into them. From there Delta tables are created from the extracted files, the main reason for delta tables is because it supports ACID transactions and rollbacks. Then the Delta tables are ported over to MYSQL using spark, from there the front-end users can leverage MYSQL and data analysts can use Delta tables. This is to ensure no resource can be bottlenecked and open up more opportunities for experienced users to leverage spark and delta tables to do their work. All the ETL, as well as CDC Jobs, are managed by airflow and run on an hourly basis&lt;/p&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;

&lt;p&gt;We immediately saw 30% more uptime as well as 40% less downtime if something does go down. A lot of Analysts are able to run their reports without fear of bringing the database down.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="Data Engineering Consulting" />
      

      

      
        <summary type="html">Sacramento County Spark Data Pipeline Project</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Project 3</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 3" />
      <published>2014-07-16T00:00:00-07:00</published>
      <updated>2014-07-16T00:00:00-07:00</updated>
      <id>http://localhost:4000/project-3</id>
      <content type="html" xml:base="http://localhost:4000/">&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;In this project I explored the idea of building serverless resources to host notebooks to run etl jobs on to export to bigquery.
From there I was able to build visualizations on Dekart and Tableau(Not shown)&lt;/p&gt;

&lt;h1 id=&quot;notebook&quot;&gt;Notebook&lt;/h1&gt;
&lt;p&gt;The notebook is hosted on a serverless spark instance within GCP. From there I downloaded a decent sized dataset (3GB) to read in, prepare the data, apply my schema and upload it to bigquery. You can see the notebook &lt;a href=&quot;https://github.com/Nicholasphom/Nicholasphom.github.io/blob/main/PortfolioCode/Project4/US_Acc_Spark.ipynb&quot;&gt;HERE&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;visualizations&quot;&gt;Visualizations&lt;/h1&gt;

&lt;p&gt;You can see the map &lt;a href=&quot;../img/portfolio/Accidents.html&quot;&gt;Here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Warning, It is big&lt;/p&gt;

&lt;h1 id=&quot;end-result&quot;&gt;End Result&lt;/h1&gt;

&lt;p&gt;This is to show how easy it is to run alot of ETL as well as Coding POCs on GCP. Where you don’t have to manage servers or clusters, everything is managed as well as it being deleted by itself automatically. You can run ETL code on a notebook, have GCP automatically run it on a schedule and then shut down saving resources.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="Web Development" />
      

      

      
        <summary type="html">Introduction</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Project 6</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 6" />
      <published>2014-07-15T00:00:00-07:00</published>
      <updated>2014-07-15T00:00:00-07:00</updated>
      <id>http://localhost:4000/project-6</id>
      <content type="html" xml:base="http://localhost:4000/">&lt;h1 id=&quot;tbd&quot;&gt;TBD&lt;/h1&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="TBD" />
      

      

      
        <summary type="html">TBD</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Project 5</title>
      <link href="http://localhost:4000/" rel="alternate" type="text/html" title="Project 5" />
      <published>2014-07-14T00:00:00-07:00</published>
      <updated>2014-07-14T00:00:00-07:00</updated>
      <id>http://localhost:4000/project-5</id>
      <content type="html" xml:base="http://localhost:4000/">&lt;p&gt;# TBD&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      
        <category term="TBD" />
      

      

      
        <summary type="html"># TBD</summary>
      

      
      
    </entry>
  
</feed>
